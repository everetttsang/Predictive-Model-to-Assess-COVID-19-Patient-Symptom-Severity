{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success - the MySageMakerInstance is in the us-west-2 region. You will use the 433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest container for your SageMaker endpoint.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import boto3, re, sys, math, json, os, sagemaker, urllib.request\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np                                \n",
    "import pandas as pd                               \n",
    "import matplotlib.pyplot as plt                   \n",
    "from IPython.display import Image                 \n",
    "from IPython.display import display               \n",
    "from time import gmtime, strftime                 \n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "\n",
    "# Define IAM role\n",
    "role = get_execution_role()\n",
    "prefix = 'sagemaker/DEMO-xgboost-dm'\n",
    "containers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com/xgboost:latest',\n",
    "              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest',\n",
    "              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com/xgboost:latest',\n",
    "              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com/xgboost:latest'} # each region has its XGBoost container\n",
    "my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "print(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + containers[my_region] + \" container for your SageMaker endpoint.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 error:  An error occurred (BucketAlreadyOwnedByYou) when calling the CreateBucket operation: Your previous request to create the named bucket succeeded and you already own it.\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'projectdatasetcovid' # <--- CHANGE THIS VARIABLE TO A UNIQUE NAME FOR YOUR BUCKET\n",
    "s3 = boto3.resource('s3')\n",
    "try:\n",
    "    if  my_region == 'us-east-1':\n",
    "      s3.create_bucket(Bucket=bucket_name)\n",
    "    else: \n",
    "      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: 201029COVID19MEXICO.csv\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  urllib.request.urlretrieve (\"https://projectdatasetcovid.s3-us-west-2.amazonaws.com/201029COVID19MEXICO.csv\",\"201029COVID19MEXICO.csv\")\n",
    "  print('Success: 201029COVID19MEXICO.csv')\n",
    "except Exception as e:\n",
    "  print('Data load error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql import SQLContext\n",
    "url = \"201029COVID19MEXICO.csv\"\n",
    "from pyspark import SparkFiles\n",
    "sc.addFile(url)\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sqlContext.read.csv(SparkFiles.get(\"201029COVID19MEXICO.csv\"), header=True, inferSchema= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FECHA_ACTUALIZACION: string (nullable = true)\n",
      " |-- ID_REGISTRO: string (nullable = true)\n",
      " |-- ORIGEN: integer (nullable = true)\n",
      " |-- SECTOR: integer (nullable = true)\n",
      " |-- ENTIDAD_UM: integer (nullable = true)\n",
      " |-- SEXO: integer (nullable = true)\n",
      " |-- ENTIDAD_NAC: integer (nullable = true)\n",
      " |-- ENTIDAD_RES: integer (nullable = true)\n",
      " |-- MUNICIPIO_RES: integer (nullable = true)\n",
      " |-- TIPO_PACIENTE: integer (nullable = true)\n",
      " |-- FECHA_INGRESO: string (nullable = true)\n",
      " |-- FECHA_SINTOMAS: string (nullable = true)\n",
      " |-- FECHA_DEF: string (nullable = true)\n",
      " |-- INTUBADO: integer (nullable = true)\n",
      " |-- NEUMONIA: integer (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- NACIONALIDAD: integer (nullable = true)\n",
      " |-- Pregnant: integer (nullable = true)\n",
      " |-- HABLA_LENGUA_INDIG: integer (nullable = true)\n",
      " |-- Native: integer (nullable = true)\n",
      " |-- DIABETES: integer (nullable = true)\n",
      " |-- EPOC: integer (nullable = true)\n",
      " |-- ASMA: integer (nullable = true)\n",
      " |-- INMUSUPR: integer (nullable = true)\n",
      " |-- HIPERTENSION: integer (nullable = true)\n",
      " |-- OTRA_COM: integer (nullable = true)\n",
      " |-- CARDIOVASCULAR: integer (nullable = true)\n",
      " |-- OBESIDAD: integer (nullable = true)\n",
      " |-- RENAL_CRONICA: integer (nullable = true)\n",
      " |-- TABAQUISMO: integer (nullable = true)\n",
      " |-- OTRO_CASO: integer (nullable = true)\n",
      " |-- TOMA_MUESTRA: integer (nullable = true)\n",
      " |-- RESULTADO_LAB: integer (nullable = true)\n",
      " |-- CLASIFICACION_FINAL: integer (nullable = true)\n",
      " |-- MIGRANTE: integer (nullable = true)\n",
      " |-- PAIS_NACIONALIDAD: string (nullable = true)\n",
      " |-- PAIS_ORIGEN: string (nullable = true)\n",
      " |-- UCI: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PATIENT_TYPE: integer (nullable = true)\n",
      " |-- SEX: integer (nullable = true)\n",
      " |-- AGE: integer (nullable = true)\n",
      " |-- INTUBED: integer (nullable = true)\n",
      " |-- NEUMONIA: integer (nullable = true)\n",
      " |-- PREGNANT: integer (nullable = true)\n",
      " |-- DIABETES: integer (nullable = true)\n",
      " |-- EPOC: integer (nullable = true)\n",
      " |-- ASTHMA: integer (nullable = true)\n",
      " |-- INMUSUPR: integer (nullable = true)\n",
      " |-- HYPERTENSION: integer (nullable = true)\n",
      " |-- CARDIOVASCULAR: integer (nullable = true)\n",
      " |-- OBESITY: integer (nullable = true)\n",
      " |-- RENAL_CRONIC: integer (nullable = true)\n",
      " |-- OTHER_DISEASE: integer (nullable = true)\n",
      " |-- TOBACCO: integer (nullable = true)\n",
      " |-- CONTACT_OTHER_COVID: integer (nullable = true)\n",
      " |-- CLASSIFICACION_FINAL: integer (nullable = true)\n",
      " |-- ICU: integer (nullable = true)\n",
      " |-- LAB_RESULT: integer (nullable = true)\n",
      " |-- DEATH: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "#df1 = df.withColumnRenamed(\"TIPO_PACIENTE\",\"Patient_Type\")\\.withColumnRenamed(\"SEXO\",\"Sex\")\n",
    "\n",
    "df1 = df.selectExpr(\"TIPO_PACIENTE as PATIENT_TYPE\",\n",
    "                   \"SEXO as SEX\",\n",
    "                   \"Age as AGE\",\n",
    "                   \"INTUBADO as INTUBED\",\n",
    "                    \"NEUMONIA as NEUMONIA\",\n",
    "                    \"Pregnant as PREGNANT\",\n",
    "                   \"DIABETES as DIABETES\",\n",
    "                  \"EPOC as EPOC\",\n",
    "                   \"ASMA as ASTHMA\",\n",
    "                   \"INMUSUPR as INMUSUPR\",\n",
    "                   \"HIPERTENSION as HYPERTENSION\",\n",
    "                   \"CARDIOVASCULAR as CARDIOVASCULAR\",\n",
    "                   \"OBESIDAD as OBESITY\",\n",
    "                   \"RENAL_CRONICA as RENAL_CRONIC\",\n",
    "                   \"OTRA_COM as OTHER_DISEASE\",\n",
    "                  \"TABAQUISMO as TOBACCO\",\n",
    "                   \"OTRO_CASO as CONTACT_OTHER_COVID\",\n",
    "                   \"CLASIFICACION_FINAL as CLASSIFICACION_FINAL\",\n",
    "                   \"UCI as ICU\",\n",
    "                   \"RESULTADO_LAB as LAB_RESULT\",\n",
    "                    \"FECHA_DEF as DEATH\"\n",
    "                  )\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---+---+-------+--------+--------+--------+----+------+--------+------------+--------------+-------+------------+-------------+-------+-------------------+--------------------+---+----------+-----+\n",
      "|PATIENT_TYPE|SEX|AGE|INTUBED|NEUMONIA|PREGNANT|DIABETES|EPOC|ASTHMA|INMUSUPR|HYPERTENSION|CARDIOVASCULAR|OBESITY|RENAL_CRONIC|OTHER_DISEASE|TOBACCO|CONTACT_OTHER_COVID|CLASSIFICACION_FINAL|ICU|LAB_RESULT|DEATH|\n",
      "+------------+---+---+-------+--------+--------+--------+----+------+--------+------------+--------------+-------+------------+-------------+-------+-------------------+--------------------+---+----------+-----+\n",
      "|           2|  2| 94|    0.0|     0.0|     0.0|     0.0| 0.0|   0.0|     0.0|         0.0|           1.0|    0.0|         0.0|          0.0|    0.0|                NaN|                   3|0.0|       1.0|    1|\n",
      "|           1|  2| 66|    NaN|     0.0|     0.0|     1.0| 0.0|   0.0|     0.0|         1.0|           0.0|    0.0|         0.0|          0.0|    0.0|                1.0|                   3|NaN|       1.0|    0|\n",
      "|           1|  2| 29|    NaN|     0.0|     0.0|     0.0| 0.0|   0.0|     0.0|         0.0|           0.0|    0.0|         0.0|          0.0|    0.0|                1.0|                   3|NaN|       1.0|    0|\n",
      "|           1|  1| 56|    NaN|     0.0|     0.0|     0.0| 0.0|   0.0|     0.0|         0.0|           0.0|    0.0|         0.0|          0.0|    0.0|                0.0|                   3|NaN|       1.0|    0|\n",
      "|           1|  1| 55|    NaN|     0.0|     0.0|     1.0| 0.0|   0.0|     0.0|         1.0|           0.0|    1.0|         0.0|          0.0|    0.0|                NaN|                   3|NaN|       1.0|    0|\n",
      "|           1|  2| 75|    NaN|     0.0|     0.0|     1.0| 0.0|   0.0|     0.0|         0.0|           1.0|    0.0|         0.0|          0.0|    0.0|                0.0|                   3|NaN|       1.0|    0|\n",
      "|           1|  2| 23|    NaN|     0.0|     0.0|     0.0| 0.0|   0.0|     0.0|         0.0|           0.0|    0.0|         0.0|          0.0|    0.0|                0.0|                   3|NaN|       1.0|    0|\n",
      "|           1|  1| 28|    NaN|     0.0|     0.0|     0.0| 0.0|   0.0|     0.0|         0.0|           0.0|    0.0|         0.0|          0.0|    0.0|                0.0|                   3|NaN|       1.0|    0|\n",
      "|           1|  1| 47|    NaN|     0.0|     0.0|     0.0| 0.0|   0.0|     0.0|         0.0|           0.0|    0.0|         0.0|          0.0|    0.0|                1.0|                   3|NaN|       1.0|    0|\n",
      "|           2|  2| 58|    0.0|     1.0|     0.0|     1.0| 1.0|   0.0|     0.0|         1.0|           0.0|    0.0|         1.0|          0.0|    0.0|                NaN|                   3|1.0|       1.0|    1|\n",
      "+------------+---+---+-------+--------+--------+--------+----+------+--------+------------+--------------+-------+------------+-------------+-------+-------------------+--------------------+---+----------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "#changed DEATH TO 1 or 0\n",
    "\n",
    "#If yes ->1, No -> 0 and Missing/NA -> 2\n",
    "df2 = df1.withColumn(\"PREGNANT\",when(col(\"PREGNANT\") == \"2\", 0).when(col(\"PREGNANT\") == \"1\", 1).when(col(\"SEX\") == \"2\", 0).otherwise(np.nan))\n",
    "df2 = df2.withColumn(\"NEUMONIA\",when(col(\"NEUMONIA\") == \"2\", 0).when(col(\"NEUMONIA\") == \"1\", 1).otherwise(np.nan))\n",
    "df2 = df2.withColumn(\"INTUBED\",when(col(\"INTUBED\") == \"2\", 0).when(col(\"INTUBED\") == \"1\", 1).otherwise(np.nan))\n",
    "df2 = df2.withColumn(\"DIABETES\",when(col(\"DIABETES\") == \"2\", 0).when(col(\"DIABETES\") == \"1\", 1).otherwise(np.nan))\n",
    "df2 = df2.withColumn(\"EPOC\",when(col(\"EPOC\") == \"2\", 0).when(col(\"EPOC\") == \"1\", 1).otherwise(np.nan))\n",
    "df2 = df2.withColumn(\"ASTHMA\",when(col(\"ASTHMA\") == \"2\", 0).when(col(\"ASTHMA\") == \"1\", 1).otherwise(np.nan))\n",
    "df2 = df2.withColumn(\"INMUSUPR\",when(col(\"INMUSUPR\") == \"2\", 0).when(col(\"INMUSUPR\") == \"1\", 1).otherwise(np.nan))\n",
    "df2 = df2.withColumn(\"HYPERTENSION\",when(col(\"HYPERTENSION\") == \"2\", 0).when(col(\"HYPERTENSION\") == \"1\", 1).otherwise(np.nan))\n",
    "df2 = df2.withColumn(\"OTHER_DISEASE\",when(col(\"OTHER_DISEASE\") == \"2\", 0).when(col(\"OTHER_DISEASE\") == \"1\", 1).otherwise(np.nan))\n",
    "df2 = df2.withColumn(\"CARDIOVASCULAR\",when(col(\"CARDIOVASCULAR\") == \"2\", 0).when(col(\"CARDIOVASCULAR\") == \"1\", 1).otherwise(np.nan))\n",
    "df2 = df2.withColumn(\"OBESITY\",when(col(\"OBESITY\") == \"2\", 0).when(col(\"OBESITY\") == \"1\", 1).otherwise(np.nan))\n",
    "df2 = df2.withColumn(\"RENAL_CRONIC\",when(col(\"RENAL_CRONIC\") == \"2\", 0).when(col(\"RENAL_CRONIC\") == \"1\", 1).otherwise(np.nan))\n",
    "df2 = df2.withColumn(\"TOBACCO\",when(col(\"TOBACCO\") == \"2\", 0).when(col(\"TOBACCO\") == \"1\", 1).otherwise(np.nan))\n",
    "#leave out contact_other_covid\n",
    "df2 = df2.withColumn(\"CONTACT_OTHER_COVID\",when(col(\"CONTACT_OTHER_COVID\") == \"2\", 0).when(col(\"CONTACT_OTHER_COVID\") == \"1\", 1).otherwise(np.nan))\n",
    "df2 = df2.withColumn(\"ICU\",when(col(\"ICU\") == \"2\", 0).when(col(\"ICU\") == \"1\", 1).otherwise(np.nan))\n",
    "df2 = df2.withColumn(\"DEATH\",when(col(\"DEATH\") == \"9999-99-99\", 0).otherwise(\"1\"))\n",
    "df2 = df2.withColumn(\"LAB_RESULT\",when(col(\"LAB_RESULT\") == \"2\", 0).when(col(\"LAB_RESULT\") == \"1\", 1).otherwise(np.nan))\n",
    "\n",
    "df2.show(10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 =df2.filter(df2[\"LAB_RESULT\"]==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+\n",
      "|LAB_RESULT| count|\n",
      "+----------+------+\n",
      "|       1.0|546940|\n",
      "+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupBy(\"LAB_RESULT\").count().sort(\"count\",ascending=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "546940\n"
     ]
    }
   ],
   "source": [
    "df2 = df2.withColumn(\"PATIENT_TYPE\",when(col(\"PATIENT_TYPE\") == \"2\", 1).when(col(\"PATIENT_TYPE\") == \"1\", 0))\n",
    "#converts the nan to unknown\n",
    "df_unknown_imputed = df2.fillna(0)\n",
    "\n",
    "df_unknown_imputed.cache()\n",
    "\n",
    "#number of entries on this dataset\n",
    "print(df_unknown_imputed.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PATIENT_TYPE: integer (nullable = true)\n",
      " |-- SEX: integer (nullable = true)\n",
      " |-- AGE: integer (nullable = true)\n",
      " |-- INTUBED: double (nullable = false)\n",
      " |-- NEUMONIA: double (nullable = false)\n",
      " |-- PREGNANT: double (nullable = false)\n",
      " |-- DIABETES: double (nullable = false)\n",
      " |-- EPOC: double (nullable = false)\n",
      " |-- ASTHMA: double (nullable = false)\n",
      " |-- INMUSUPR: double (nullable = false)\n",
      " |-- HYPERTENSION: double (nullable = false)\n",
      " |-- CARDIOVASCULAR: double (nullable = false)\n",
      " |-- OBESITY: double (nullable = false)\n",
      " |-- RENAL_CRONIC: double (nullable = false)\n",
      " |-- OTHER_DISEASE: double (nullable = false)\n",
      " |-- TOBACCO: double (nullable = false)\n",
      " |-- CONTACT_OTHER_COVID: double (nullable = false)\n",
      " |-- CLASSIFICACION_FINAL: integer (nullable = true)\n",
      " |-- ICU: double (nullable = false)\n",
      " |-- LAB_RESULT: double (nullable = false)\n",
      " |-- DEATH: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "df_unknown_imputed.printSchema()\n",
    "df_unknown_imputed = df_unknown_imputed.withColumn(\"DEATH\", df_unknown_imputed[\"DEATH\"].cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinctDF = df_unknown_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Considering Patient_type is determined we considered if ICU care is needed or not\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, VectorAssembler\n",
    "categoricalColumns = ['PATIENT_TYPE','SEX','NEUMONIA', 'PREGNANT','DIABETES', 'EPOC', 'ASTHMA', 'INMUSUPR','HYPERTENSION','OTHER_DISEASE',\n",
    "           'CARDIOVASCULAR','OBESITY','RENAL_CRONIC','TOBACCO']\n",
    "stages = []\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "    stages += [stringIndexer, encoder]\n",
    "label_stringIdx = StringIndexer(inputCol = 'ICU', outputCol = 'label')\n",
    "stages += [label_stringIdx]\n",
    "numericCols = ['AGE']\n",
    "assemblerInputs = [c + \"classVec\" for c in categoricalColumns] + numericCols\n",
    "assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "stages += [assembler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- PATIENT_TYPE: integer (nullable = true)\n",
      " |-- SEX: integer (nullable = true)\n",
      " |-- AGE: integer (nullable = true)\n",
      " |-- INTUBED: double (nullable = false)\n",
      " |-- NEUMONIA: double (nullable = false)\n",
      " |-- PREGNANT: double (nullable = false)\n",
      " |-- DIABETES: double (nullable = false)\n",
      " |-- EPOC: double (nullable = false)\n",
      " |-- ASTHMA: double (nullable = false)\n",
      " |-- INMUSUPR: double (nullable = false)\n",
      " |-- HYPERTENSION: double (nullable = false)\n",
      " |-- CARDIOVASCULAR: double (nullable = false)\n",
      " |-- OBESITY: double (nullable = false)\n",
      " |-- RENAL_CRONIC: double (nullable = false)\n",
      " |-- OTHER_DISEASE: double (nullable = false)\n",
      " |-- TOBACCO: double (nullable = false)\n",
      " |-- CONTACT_OTHER_COVID: double (nullable = false)\n",
      " |-- CLASSIFICACION_FINAL: integer (nullable = true)\n",
      " |-- ICU: double (nullable = false)\n",
      " |-- LAB_RESULT: double (nullable = false)\n",
      " |-- DEATH: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "cols = distinctDF.columns\n",
    "pipeline = Pipeline(stages = stages)\n",
    "pipelineModel = pipeline.fit(distinctDF)\n",
    "distinctDF = pipelineModel.transform(distinctDF)\n",
    "selectedCols = ['label', 'features'] + cols\n",
    "distinctDF = distinctDF.select(selectedCols)\n",
    "distinctDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dataset Count: 382200\n",
      "Test Dataset Count: 164740\n"
     ]
    }
   ],
   "source": [
    "train, test = distinctDF.randomSplit([0.7, 0.3], seed = 2018)\n",
    "print(\"Training Dataset Count: \" + str(train.count()))\n",
    "print(\"Test Dataset Count: \" + str(test.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "lr = LogisticRegression(featuresCol = 'features', labelCol = 'label', maxIter=10)\n",
    "lrModel = lr.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------+-----+--------------------+----------+--------------------+\n",
      "|AGE|SEX|PATIENT_TYPE|label|       rawPrediction|prediction|         probability|\n",
      "+---+---+------------+-----+--------------------+----------+--------------------+\n",
      "| 71|  2|           0|  0.0|[7.0651362537242,...|       0.0|[0.99914635083931...|\n",
      "| 82|  2|           0|  0.0|[7.72683301273775...|       0.0|[0.99955935621541...|\n",
      "| 28|  2|           0|  0.0|[8.28170739259796...|       0.0|[0.99974695936392...|\n",
      "| 43|  2|           0|  0.0|[8.18369905639312...|       0.0|[0.99972091054067...|\n",
      "| 70|  2|           0|  0.0|[5.25248296482816...|       0.0|[0.99479275220747...|\n",
      "| 80|  2|           0|  0.0|[5.45554693542275...|       0.0|[0.99574563759130...|\n",
      "| 54|  1|           0|  0.0|[8.06045945419492...|       0.0|[0.99968431800184...|\n",
      "| 44|  1|           0|  0.0|[7.06016344728858...|       0.0|[0.99914209888578...|\n",
      "| 47|  1|           0|  0.0|[7.97262852536261...|       0.0|[0.99965534727341...|\n",
      "| 56|  1|           0|  0.0|[7.35665971771548...|       0.0|[0.99936208010642...|\n",
      "+---+---+------------+-----+--------------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Pateint_type for ICU is also observed and it shoud ideally be 1 meaning hospitlaised if prediction is 1\n",
    "predictions = lrModel.transform(test)\n",
    "predictions.select('AGE', 'SEX','PATIENT_TYPE', 'label', 'rawPrediction', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC 0.9129017943795279\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluator = BinaryClassificationEvaluator()\n",
    "print('Test Area Under ROC', evaluator.evaluate(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------+-----+--------------+----------+-----------+\n",
      "|AGE|SEX|PATIENT_TYPE|label| rawPrediction|prediction|probability|\n",
      "+---+---+------------+-----+--------------+----------+-----------+\n",
      "| 71|  2|           0|  0.0|[285535.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "| 82|  2|           0|  0.0|[285535.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "| 28|  2|           0|  0.0|[285535.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "| 43|  2|           0|  0.0|[285535.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "| 70|  2|           0|  0.0|[285535.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "| 80|  2|           0|  0.0|[285535.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "| 54|  1|           0|  0.0|[285535.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "| 44|  1|           0|  0.0|[285535.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "| 47|  1|           0|  0.0|[285535.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "| 56|  1|           0|  0.0|[285535.0,0.0]|       0.0|  [1.0,0.0]|\n",
      "+---+---+------------+-----+--------------+----------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxDepth = 3)\n",
    "dtModel = dt.fit(train)\n",
    "predictions = dtModel.transform(test)\n",
    "predictions.select('AGE', 'SEX', 'PATIENT_TYPE','label', 'rawPrediction', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.9063333097751165\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------+-----+--------------------+----------+--------------------+\n",
      "|AGE|SEX|PATIENT_TYPE|label|       rawPrediction|prediction|         probability|\n",
      "+---+---+------------+-----+--------------------+----------+--------------------+\n",
      "| 71|  2|           0|  0.0|[19.8948054770550...|       0.0|[0.99474027385275...|\n",
      "| 82|  2|           0|  0.0|[19.8482550826058...|       0.0|[0.99241275413029...|\n",
      "| 28|  2|           0|  0.0|[19.9626168224299...|       0.0|[0.99813084112149...|\n",
      "| 43|  2|           0|  0.0|[19.9164900618665...|       0.0|[0.99582450309332...|\n",
      "| 70|  2|           0|  0.0|[19.5019972708563...|       0.0|[0.97509986354281...|\n",
      "| 80|  2|           0|  0.0|[19.6383878740054...|       0.0|[0.98191939370027...|\n",
      "| 54|  1|           0|  0.0|[19.4359583686047...|       0.0|[0.97179791843023...|\n",
      "| 44|  1|           0|  0.0|[19.9149517510683...|       0.0|[0.99574758755341...|\n",
      "| 47|  1|           0|  0.0|[19.8614000174145...|       0.0|[0.99307000087072...|\n",
      "| 56|  1|           0|  0.0|[19.8465443650985...|       0.0|[0.99232721825492...|\n",
      "+---+---+------------+-----+--------------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf = RandomForestClassifier(featuresCol = 'features', labelCol = 'label')\n",
    "rfModel = rf.fit(train)\n",
    "predictions = rfModel.transform(test)\n",
    "predictions.select('AGE', 'SEX','PATIENT_TYPE','label', 'rawPrediction', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.9159009823121372\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+------------+-----+--------------------+----------+--------------------+\n",
      "|AGE|SEX|PATIENT_TYPE|label|       rawPrediction|prediction|         probability|\n",
      "+---+---+------------+-----+--------------------+----------+--------------------+\n",
      "| 71|  2|           0|  0.0|[1.32590267922052...|       0.0|[0.93412217565280...|\n",
      "| 82|  2|           0|  0.0|[1.32590267922052...|       0.0|[0.93412217565280...|\n",
      "| 28|  2|           0|  0.0|[1.32590267922052...|       0.0|[0.93412217565280...|\n",
      "| 43|  2|           0|  0.0|[1.32590267922052...|       0.0|[0.93412217565280...|\n",
      "| 70|  2|           0|  0.0|[1.32590267922052...|       0.0|[0.93412217565280...|\n",
      "| 80|  2|           0|  0.0|[1.32590267922053...|       0.0|[0.93412217565280...|\n",
      "| 54|  1|           0|  0.0|[1.32590267922055...|       0.0|[0.93412217565281...|\n",
      "| 44|  1|           0|  0.0|[1.32590267922052...|       0.0|[0.93412217565280...|\n",
      "| 47|  1|           0|  0.0|[1.32590267922052...|       0.0|[0.93412217565280...|\n",
      "| 56|  1|           0|  0.0|[1.32590267922052...|       0.0|[0.93412217565280...|\n",
      "+---+---+------------+-----+--------------------+----------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import GBTClassifier\n",
    "gbt = GBTClassifier(maxIter=10)\n",
    "gbtModel = gbt.fit(train)\n",
    "predictions = gbtModel.transform(test)\n",
    "predictions.select('AGE', 'SEX', 'PATIENT_TYPE','label', 'rawPrediction', 'prediction', 'probability').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Area Under ROC: 0.9174592579701456\n"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator()\n",
    "print(\"Test Area Under ROC: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
